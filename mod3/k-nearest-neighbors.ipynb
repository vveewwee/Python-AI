{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac78609",
   "metadata": {},
   "source": [
    "# POC - AI Pool 2022 - Day 01 - KNN\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Algorithm k-nearest-neighbors\n",
    "\n",
    "The **k-nearest neighbors (KNN) algorithm** is a data classification method for estimating **the likelihood** that a data point will become a member of one group or another based on what group the data points nearest to it belong to.\n",
    "It is a type of **supervised machine learning algorithm** used to solve classification and regression problems. However, it's mainly used for classification problems.\n",
    "\n",
    "KNN is a **lazy learning and non-parametric algorithm.**\n",
    "\n",
    "It's called a lazy learning algorithm or lazy learner because it doesn't perform any training when you supply the training data. Instead, it just stores the data during the training time and doesn't perform any calculations. It doesn't build a model until a query is performed on the dataset\n",
    "\n",
    "**docs:** [How kNN algorithm works](https://www.youtube.com/watch?v=UqYde-LULfs)\n",
    "\n",
    "### What are you going to do?\n",
    "\n",
    "In a first step, you will get a dataset in csv format, representing different categories of flowers, as well as data on these flowers, you will process the dataset in order to have optimized and clean data.\n",
    "Then you will develop tools to evaluate our future model, finally you will build your own KNN algorithm and you will compare it to the one proposed by the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc9f23",
   "metadata": {},
   "source": [
    "## I) Data preprocessing\n",
    "\n",
    "### Import\n",
    "For the first cell, we import functions that you will have to use to create your KNN algorithm :\n",
    "\n",
    "**from** random **import** [seed](https://www.w3schools.com/python/ref_random_seed.asp)\\\n",
    "**from** random **import** [randrange](https://www.geeksforgeeks.org/randrange-in-python/)\\\n",
    "**from** csv **import** [reader](https://docs.python.org/3/library/csv.html)\\\n",
    "**from** math **import** [sqrt](https://www.geeksforgeeks.org/python-math-function-sqrt/)\n",
    "    \n",
    "Think to read, the documentations to impregnate you of the functions and their mode of use.\\\n",
    "But don't worry, I will tell you **when you should use** the imported functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d006bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions with k-nearest neighbors on the Iris Flowers Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c05963",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "To start the subject, I remind you that we are going to make a **KNN from scratch**,\\\n",
    "but we are also going to create **all our tools from scratch** so hang on tight!\n",
    "\n",
    "You should have in your folder where this notebook is, a folder named ``data`` which contains a file named ``iris.csv``.\\\n",
    "If it is not the case I invite you to **contact a supervisor**.\\\n",
    "This file represents the dataset that we will use.\n",
    "\n",
    "It is a **table** with 5 columns :\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. class: [Iris Setosa, Iris Versicolour, Iris Virginica]\n",
    "\n",
    "**Exercice:**\\\n",
    "Complete the **load_dataset function** in order to get a dataset which will be **a list of lists** where each list will be a row of our csv file.\n",
    "You **should use the reader function** import earlier in order to be able to read the csv file correctly.\n",
    "To validate the exercise you just have to **validate the assert** of the cell which will confirm that your function has the right behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f23b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/iris.csv\"\n",
    "\n",
    "# Load a CSV file\n",
    "def load_dataset(filename: str) -> List[Tuple[str, str, str, str, str]]:\n",
    "    dataset = []\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(filename)\n",
    "assert len(dataset) > 0 and dataset[0] == ['5.1', '3.5', '1.4', '0.2', 'Iris-setosa'], \"You failed to load the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea0d4f",
   "metadata": {},
   "source": [
    "### String numerical values to float\n",
    "\n",
    "Congratulations, you **have successfully loaded a dataset from a csv.**\\\n",
    "This is a function that **you can re-use** in all sorts of applications that require loading datasets.\n",
    "\n",
    "However, if you look at the assert of the cell above, **you can see that the numbers are strings**, and that's problematic because you won't be able to use them as numbers.\n",
    "\n",
    "It looks like we'll have to develop a function to **transform these values into floats.**\n",
    "\n",
    "**Exercise :**\\\n",
    "Develop a function that allows you to transform float values that are strings into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(filename)\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset):\n",
    "    pass\n",
    "\n",
    "str_column_to_float(dataset)\n",
    "assert len(dataset) > 0 and dataset[0] == [5.1, 3.5, 1.4, 0.2, 'Iris-setosa'], \"The values should be float.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9aafbe",
   "metadata": {},
   "source": [
    "### String labels to integer values\n",
    "\n",
    "We are still missing one thing in our dataset, that is to **transform the values of the different classes into integer values**, this will greatly facilitate the task of evaluating our classification algorithm.\n",
    "\n",
    "Here are the **different classes and their corresponding values:**\n",
    "\n",
    " * \"Iris-setosa\" &#8594; 0\n",
    " * \"Iris-versicolor\" &#8594; 1\n",
    " * \"Iris-virginica\" &#8594; 2\n",
    " \n",
    "**Exercice :**\\\n",
    "Replace the different labels with their full value listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(filename)\n",
    "str_column_to_float(dataset)\n",
    "    \n",
    "# Convert string column to float\n",
    "def str_column_to_integer(dataset, column: int, values = {\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2}):\n",
    "    pass\n",
    "\n",
    "str_column_to_integer(dataset, len(dataset[0])-1)\n",
    "assert len(dataset) > 136 and dataset[137] == [6.4, 3.1, 5.5, 1.8, 2], \"The label column should be replace per integer values.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d9e09",
   "metadata": {},
   "source": [
    "### Normalize data\n",
    "\n",
    "#### I) Normalize data - find minmax values\n",
    "\n",
    "**Normalization** is a process of reporting data on a certain scale which is usually 0&#8594;1.\n",
    "\n",
    "**Exemple :**\n",
    "\n",
    " * [1, 3, 5] &#8594; **[0.2, 0.6, 1]**\n",
    " * [1, 2, 4] &#8594; **[0.25, 0.5, 1]**\n",
    " * [1, 100, 33] &#8594; **[0.1, 1.0, 0.33]**\n",
    "\n",
    "When we realize neural networks **we tend to normalize the data between 0 and 1, which allows the algorithm to reduce its error more quickly.** However, one might wonder why one should normalize data on which one is just going to calculate distances, in fact **if the dataset was based on a single dimension normalization would be useless except to reduce the scale.**\n",
    "\n",
    "Here is **an example of the usefulness of normalization on a two dimensional dataset:**\n",
    "\n",
    "<p align=\"center\">\n",
    "   <img src=\"./images/normalization.png\" alt=\"Normalization\">\n",
    "</p>\n",
    " \n",
    "We will split the normalization into two parts:\n",
    " \n",
    "  * Find **the minimum and maximum** of each column.\n",
    "  * **Scale our data** using the minimum and maximum of each column.\n",
    "  \n",
    "Come on, let's go!\n",
    " \n",
    "**Exercice :**\\\n",
    "Find the minimum and maximum of each column and return that as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6529cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the min and max values for each column\n",
    "\n",
    "def data_minmax(dataset) -> List[List[str]]:\n",
    "    minmax = []\n",
    "    return None\n",
    "\n",
    "assert data_minmax(dataset) == [[4.3, 7.9], [2.0, 4.4], [1.0, 6.9], [0.1, 2.5]], \"You have to get the minimum and maximum values of each column.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd9336",
   "metadata": {},
   "source": [
    "#### II) Normalize data - scale data\n",
    "\n",
    "the goal of normalization is to **change the values of numeric columns in the dataset to a common scale**, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization. **It is required only when** features have different ranges.\n",
    "\n",
    "**Exercice :**\\\n",
    "Program a function that **scales the dataset values to 0&#8594;1,**.\\\n",
    "**You won't scale the values of the classes** that correspond to the different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset.copy()\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax: List[List[str]]):    \n",
    "    pass\n",
    "\n",
    "normalize_dataset(dataset_copy, data_minmax(dataset_copy))\n",
    "assert len(dataset_copy) > 0 and dataset_copy[0] == [0.22222222222222213, 0.6249999999999999, 0.06779661016949151, 0.04166666666666667, 0], \"The values should be between 0 and 1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32124755",
   "metadata": {},
   "source": [
    "## II) Evaluation & K-fold Cross-Validation\n",
    "\n",
    "### K-fold cross validation\n",
    "\n",
    "Cross-validation is a **resampling procedure** used to **evaluate machine learning models on a limited data sample.**\n",
    "\n",
    "The procedure has **a single parameter called k that refers to the number of groups** that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "Cross-validation is primarily used **in applied machine learning to estimate the skill of a machine learning model on unseen data.** That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "\n",
    "It is **a popular method** because it is simple to understand and because **it generally results in a less biased or less optimistic estimate of the model skill** than other methods, such as a simple train/test split.\n",
    "\n",
    "**The general procedure** is as follows:\n",
    "\n",
    " 1. **Shuffle** the dataset randomly.\n",
    " 2. **Split** the dataset into k groups\n",
    " 3. For each unique group:\n",
    "    1. **Take the group** as a hold out or test data set\n",
    "    2. **Take the remaining groups** as a training data set**\n",
    "    3. **Fit a model** on the training set and evaluate it on the test set**\n",
    "    4. **Retain the evaluation score** and discard the model**\n",
    " 4. **Summarize** the skill of the model using the sample of model evaluation scores\n",
    " \n",
    "**Exercice :**\\\n",
    "Develop a function that will shuffle the dataset, and create n_folds subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd75cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds: int):\n",
    "    return None\n",
    "\n",
    "folds = cross_validation_split(dataset, n_folds)\n",
    "assert len(folds) == 5 and len(folds[0]) == 30, \"You have to split the dataset into 5 parts, equitably divided.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4306a",
   "metadata": {},
   "source": [
    "### Accuracy metric\n",
    "\n",
    "We will start by **defining the terms** ``metric`` and ``accuracy`` :\n",
    " * **Metric** evaluates **the quality of an engine** by comparing engine's output (predicted result) with the original label (actual result).\n",
    " * The **accuracy metric** calculates **how often predictions equal labels.**\n",
    " \n",
    "In plain terms, **accuracy is your percentage* of correct predictions out of all predictions.**\n",
    "\n",
    "**Exercice :**\\\n",
    "Complete the function to **obtain the accuracy in percentage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a26cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [0, 1, 0, 1, 2, 1, 2, 1]\n",
    "predicted = [0, 2, 0, 1, 2, 2, 2, 0]\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual: List[int], predicted: List[int]) -> float:\n",
    "    return None\n",
    "\n",
    "acc = accuracy_metric(actual, predicted)\n",
    "assert acc == 62.5, \"Your function does not return the percentage of accuracy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5bf1e",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For the evaluation function, **it is quite complex** so you won't have to develop it, but it is important that **you understand how it works.**\n",
    "\n",
    "We will look at it step by step\n",
    "\n",
    "   * First **we split our dataset** in n_folds subsets, in order to have validation data.\n",
    "   * We initialize a list of scores, in fact **we will get a score for each subset.**\n",
    "   * Then we create a loop for each subset **to test our model on each subset.**\n",
    "   * We create the training data called train_set, it corresponds to **all subsets except the one we test**, the training data are the potential neighbors.\n",
    "   * We remove from the training data the file we want to test.\n",
    "   * then in the following loop we will add line by line in a list which is the test subset, **taking care to remove the labels**, indeed we want to predict them so we must not keep them during the test.\n",
    "   * then **we get the predictions from your algorithm**, this corresponds to each predicted value for each line of our test subset.\n",
    "   * we recover the labels of the test subset in order to compare them with our predictions thanks to the accuracy_metric function that we developed and then we add the accuracy to our score list, and then we go on to test a next subset.\n",
    "    \n",
    "Take the time to read and reread until you have no more questions. In case you still have some unclear areas I invite you to join a manager in the question channels in order to establish the functioning of the evaluation function, it is the only one that you do not develop but may be the most important since it allows to measure the effectiveness of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds) # get folds for K-fold Cross-Validation\n",
    "    scores = [] # we initialize our score list, we should have at the end as many scores as fold.\n",
    "    for fold in folds: # we will evaluate model for each fold.\n",
    "        train_set = list(folds) # we create the train set\n",
    "        train_set.remove(fold)  # we delete the file that will be tested from the train dataset\n",
    "        train_set = sum(train_set, []) # we concatenate all the training folds.\n",
    "        test_set = []\n",
    "        for row in fold: # # we create the test set\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy) # we add each line of the tested fold in the test set\n",
    "            row_copy[-1] = None # We remove the labels from the test set since we have to predict them.\n",
    "        predicted = algorithm(train_set, test_set, *args) # on recupere les predictions du fold de test\n",
    "        labels = [row[-1] for row in fold] # we retrieve the predictions of the test fold\n",
    "        accuracy = accuracy_metric(labels, predicted) # the precision is calculated from the labels and the predictions.\n",
    "        scores.append(accuracy) # we add the score of the tested file to our score list.\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62be5e2",
   "metadata": {},
   "source": [
    "## III) K-nearest-neighbors algorithm\n",
    "\n",
    "### Euclidean distance\n",
    "\n",
    "The first step is to calculate the distance between two rows in a dataset.\n",
    "\n",
    "To calculate the distance between two rows (your new sample and all the data you have in your dataset) is very simple,there are several ways to get this value, in this exercice we will use **the Euclidean distance.**\n",
    "\n",
    "Rows of data are mostly made up of numbers and an easy way to calculate the distance between two rows or vectors of numbers is **to draw a straight line.** This makes sense in 2D or 3D and scales nicely to higher dimensions.\n",
    "\n",
    "**We can calculate the straight line distance between two vectors using the Euclidean distance measure**. It is calculated as the square root of the sum of the squared differences between the two vectors.\n",
    "\n",
    "But **there are many metrics for calculating distances**, even if the Euclidean distance remains the most popular metric, sometimes another metric will fit better to our problem.\n",
    "\n",
    "For the moment, **we will develop a function to compute the euclidean distance.**\n",
    "\n",
    "Here is the formula:\n",
    "\n",
    "$$d\\left( p,q\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( q_{i}-p_{i}\\right)^2 }$$\n",
    "\n",
    "**Exerice :**\\\n",
    "Develop a function that calculates the distance between two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = [4.5, 2.3, 1.3, 0.4, 1]\n",
    "row2 = [3.5, 4.3, 3.3, 0.4, 1]\n",
    "\n",
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1: List[int], row2: List[int]) -> float:\n",
    "    return None\n",
    "\n",
    "distance = euclidean_distance(row1, row2)\n",
    "assert distance == 3.0, \"Your distance calcul is wrong, look at the formula.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ade232",
   "metadata": {},
   "source": [
    "### Neighbors\n",
    "\n",
    "Neighbors for a new piece of data in the dataset are **the k closest instances**, as defined by our distance measure.\n",
    "\n",
    "To locate the neighbors for a new piece of data within a dataset **we must first calculate the distance between each record in the dataset to the new piece of data**. We can do this using our distance function prepared above.\n",
    "\n",
    "Once distances are calculated, **we must sort all of the records in the training dataset by their distance to the new data.** We can then select **the top k to return as the most similar neighbors.**\n",
    "\n",
    "We can do this by keeping track of the distance for each record in the dataset as a tuple, sort the list of tuples by the distance (in descending order) and then retrieve the neighbors.\n",
    "\n",
    "**Exercice :**\\\n",
    "Get all the neighbors of ``test_row`` that are in ``train`` and their distances.\\\n",
    "Sort the neighbors by distance to get the closest ``num_neighbors`` and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row = [3.5, 4.3, 3.3, 0.8]\n",
    "fake_fold = [[1.5, 2.3, 3.3, 4, 1], [4.6, 2.3, 1.88, 0.15, 2], [3.2, 4.1, 3.2, 0.3, 0], [2.5, 1.3, 3.8, 2, 1], [3.1, 4.4, 9.2, 5.3, 0]]\n",
    "\n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train: List[List[int]], test_row: List[int], num_neighbors: int) -> List[List[int]]:\n",
    "    return None\n",
    "\n",
    "neighbors = get_neighbors(fake_fold, test_row, 2)\n",
    "assert len(neighbors) > 0 and neighbors[1] == [4.6, 2.3, 1.88, 0.15, 2], \"You should get the nearest neighbors of test_row.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cea20b",
   "metadata": {},
   "source": [
    "### Predict classification\n",
    "\n",
    "**The most similar neighbors collected from the training dataset can be used to make predictions.**\n",
    "\n",
    "In the case of classification, we can return the most represented class among the neighbors.\n",
    "\n",
    "We can achieve this by performing the max() function on the list of output values from the neighbors. Given a list of class values observed in the neighbors, the max() function takes a set of unique class values and calls the count on the list of class values for each class value in the set\n",
    "\n",
    "**Exercice :**\\\n",
    "**Get neighbors** with the previous function.\\\n",
    "Make a prediction based on **the type of neighbor most present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d74c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    return None\n",
    "\n",
    "assert predict_classification(fake_fold, test_row, 2) == 0, \"You should predict a class, according to the nearest neighbors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902bcee",
   "metadata": {},
   "source": [
    "#### Knn algorithm\n",
    "\n",
    "Now **we have everything we need to build our knn algorithm**. In fact we don't have much left to do since we can get the predictions from the previous function.\n",
    "\n",
    "We are going to predict the value of each test data according to the training data. After that we return the set of predictions, here is our knn algorithm.\n",
    "\n",
    "**Exercice :**\\\n",
    "For each row in the test data, **get the prediction and put it in a list.**\\\n",
    "Finally **return this list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_fold = [[1.5, 2.3, 3.3, 4, 1], [4.6, 2.3, 1.88, 0.15, 1], [3.2, 4.1, 3.2, 0.3, 0], [2.5, 1.3, 3.8, 2, 2]]\n",
    "fake_fold2 = [[3.5, 1.3, 3.3, 4], [4.6, 2.3, 1.88, 0.15], [3.2, 4.1, 3.2, 0.3], [2.5, 1.3, 3.8, 2]]\n",
    "\n",
    "def k_nearest_neighbors(train, test, num_neighbors: int) -> List[List[int]]:\n",
    "    return None\n",
    "\n",
    "assert k_nearest_neighbors(fake_fold, fake_fold2, 2)[3] == 1, \"You should get prediction for each test row\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda93ea",
   "metadata": {},
   "source": [
    "#### Use your algorithm\n",
    "\n",
    "This section **applies your KNN algorithm** to the Iris flowers dataset.\n",
    "\n",
    "The first step is to **load the dataset** and **convert the loaded data to numbers**. For this we will use the helper function ``load_csv()`` to load the file, ``str_column_to_float()`` to convert string numbers to floats and ``str_column_to_integer()`` to convert the class column to integer values.\n",
    "\n",
    "We will evaluate the algorithm using **k-fold cross-validation with 5 folds.** This means that 150/5=30 records will be in each fold. We will use the helper functions ``evaluate_algorithm()`` to evaluate the algorithm with cross-validation and ``accuracy_metric()`` to calculate the accuracy of predictions.\n",
    "\n",
    "You will use your function named ``k_nearest_neighbors()`` to manage the application of the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33187d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "seed(1)\n",
    "\n",
    "filename = \"./data/iris.csv\"\n",
    "dataset = None\n",
    "\n",
    "# convert features into float -> call your function\n",
    " \n",
    "# convert class column to integers -> call your function\n",
    "\n",
    "# normalize your dataset\n",
    "minmax = None\n",
    "\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "\n",
    "score_my_knn = sum(scores)/float(len(scores))\n",
    "print('Mean Accuracy: %.3f%%' % score_my_knn)\n",
    "\n",
    "assert score_my_knn >= 90 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697d086",
   "metadata": {},
   "source": [
    "**You should have an average accuracy above 90** but it is likely that your accuracy will be above 95 if everything went well, however it should not exceed 98% at the most, or else there is certainly an error at some point.\n",
    "\n",
    "**In any case you have done the hard part**, you have developed a knn algorithm from scratch.\n",
    "\n",
    "Now we will move to a part that should be simpler but more pleasant, since **we will compare your algorithm to the one proposed by the Scikit-learn library.**\n",
    "\n",
    "**Scikit-learn is a very popular library** that provides machine learning models.\n",
    "\n",
    "![sklearn](images/sklearn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0dcd1",
   "metadata": {},
   "source": [
    "## IV) Scikit-learn KNN algorithm\n",
    "\n",
    "#### Import\n",
    "\n",
    "We import different functions from scikit-learn, I let you read the documentations.\\\n",
    "But I have no doubt that if you have spent time on the previous exercises you should quickly understand the purpose of these functions.\n",
    "\n",
    " * [load_iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)\n",
    " * [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    " * [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    " * [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fcb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163bc29",
   "metadata": {},
   "source": [
    "#### Load dataset\n",
    "\n",
    "The iris dataset is a classic and very easy multi-class classification dataset.\\\n",
    "Scikit-learn allows to download this dataset with its function.\n",
    "\n",
    "[load_iris()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)\n",
    "\n",
    "**Exercice :**\\\n",
    "Use the ``load_iris`` function to load dataset in ``iris`` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load iris dataset with the load_iris function\n",
    "iris = None\n",
    "assert iris['data'][0][1] == 3.5, \"Use the load_iris function() to get the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e69df3",
   "metadata": {},
   "source": [
    "#### Get data\n",
    "\n",
    "The dataset provided by scikit-learn **does not need any particular pre-processing.**\\\n",
    "Except that it could be normalized, but this is not necessary for the iris dataset problem.\n",
    "\n",
    "The dataset **contains two interesting attributes** ``target`` and ``data``.\n",
    "\n",
    "**Exercise:**\\\n",
    "Learn about these two attributes ``iris.target`` and ``iris.data``.\\\n",
    "Assign to ``X`` the different data while ``y`` will get the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b17e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = None\n",
    "X = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23b94c",
   "metadata": {},
   "source": [
    "#### Split data in test and train subset\n",
    "\n",
    "We will not use the K-fold cross validation method to recover the accuracy of the model proposed by scikit-learn.\n",
    "\n",
    "Instead I propose to **create a test subset** using the function ``train_test_split()`` of scikit-learn, with a test subset of **size 0.2** or length equal to 30 while the training data will correspond to 80% of the dataset or a length equal to 120.\n",
    "\n",
    "We will use a **random_state equal to 4.**\n",
    "\n",
    "[train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "**Exercice :**\\\n",
    "Use the function ``train_test_split`` to get ``X_train``, ``X_test``, ``y_train``, ``y_test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the train_test_split function\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "assert X_train.shape == (120, 4) and X_test.shape == (30, 4) and y_train.shape == (120,) and y_test.shape == (30,), \"You dit not split the data like 80/20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012e574",
   "metadata": {},
   "source": [
    "#### KNeighborsClassifier\n",
    "\n",
    "Now **create an algorithm knn** using the function ``KNeighborsClassifier()`` which will have as parameter ``k = 5``.\\\n",
    "Then train this algorithm with the ``KNeighborsClassifier.fit(x, y)`` method and replace x and y per training data.\\\n",
    "**The training consists in storing all the training data** in order to be able to use them as potential neighbors during the predictions.\n",
    "\n",
    "[KNeighborsClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "**Exercice :**\n",
    "Create a knn algorithm using the function proposed by scikit-learn.\\\n",
    "Train your algorithm with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KNN from sklearn\n",
    "knn = None\n",
    "\n",
    "assert knn.predict([[4.6, 2.3, 1.88, 2.15]])[0] == 1, \"The knn from scikit learn should predict 1 in this configuration.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd9441",
   "metadata": {},
   "source": [
    "#### Evaluate scikit-learn knn\n",
    "\n",
    "Now we just need to **get the predictions** using the method ``KNeighborsClassifier.predict(x)``, replace x by the test data.\n",
    "\n",
    "Then **apply the metric** ``accuracy_score(y_test, y_pred)`` in order to obtain the accuracy of the scikit-learn model.\n",
    "\n",
    "[accuracy_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "\n",
    "**Exerice :**\\\n",
    "**Retrieve the predictions** of scikit-learn's knn algorithm and **calculate the accuracy** of their model.\\\n",
    "The scikit-learn model should pass the 90% accuracy mark with flying colors.2\\\n",
    "``hint``: multiply your accuracy per 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy\n",
    "y_pred = None\n",
    "score_sklearn = None\n",
    "\n",
    "assert score_sklearn > 90, \"You missed something i guess.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab5349",
   "metadata": {},
   "source": [
    "#### Your algorithm versus scikit-learn's\n",
    "\n",
    "Now the accuracy of your model is displayed below that of sci-kit learn, normally you should not be far away or even equal.\n",
    "\n",
    "Do you realize that you have remade from scratch an algorithm proposed by a popular machine learning library, **Bravo!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scikit-Learn KNN : [%.2f%%]\\nHomemade KNN : [%.2f%%]\\n\" % (score_sklearn, score_my_knn))\n",
    "assert score_sklearn > 90 and score_my_knn > 90, \"One of your model seems wrong.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b07069",
   "metadata": {},
   "source": [
    "### Bravo\n",
    "\n",
    "If you made it this far, you should have succeeded in reproducing a KNN from scratch algorithm that matches the KNN algorithm proposed by the Scikit-Learn library.\n",
    "\n",
    "Feel free to use this algorithm to solve machine learning problems in the future, you now master this algorithm perfectly.\n",
    "\n",
    "You can be proud of yourself, in the next topic we will see the Bayesian methods, a significant part of machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
