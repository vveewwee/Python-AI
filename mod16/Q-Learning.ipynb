{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym --user\n",
    "!pip3 install numpy --user\n",
    "!pip3 install matplotlib --user\n",
    "\n",
    "!pip3 install box2d --user\n",
    "!pip3 install pyvirtualdisplay -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "Dans le précédent workshop nous avions vue les bases du **Reinforcement learning** en resolvant L-Antique Maze via la value function. <br>\n",
    "Ça été l'occasion de découvrir les diverses notions mathematiques derriere le RL (MDP, VF, etc)\n",
    "\n",
    "Dans ce workshop, vous allez apprendre les bases de la notion du **Q-learning** et decouvrir les environnements Gym.<br>\n",
    "Vous allez pour cela à la fin de ce workshop résoudre un environnement nommé [MontainCar](gym.openai.com/envs/MountainCar-v0/).\n",
    "\n",
    "### Packages\n",
    "Importons dans un premier temps les dépences suivantes:  \n",
    "-numpy est le package fondamental pour le calcul scientifique avec Python.  \n",
    "-matplotlib  est une librairie connue pour afficher des graphiques en Python.  \n",
    "-[gym](https://pypi.org/project/gym/0.7.4/) est un outil utile lors d'usage de reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from validation_tests import *\n",
    "from pyvirtualdisplay import Display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Prise en main avec l'environnement\n",
    "\n",
    "Avant de commencer l'implémentation du Q-learning, commençons par nous familiariser avec la librairie `gym`.\n",
    "Gym vous permet de tester votre agent dans un environnement. Les environnements fournis sont variés et de diverses complexités.\n",
    "\n",
    "Celui d'aujourd'hui est nommé `MountainCar-V0`, il consiste en un vehicule situé au creux d'une colline ayant pour but de la franchir.<br>\n",
    "\n",
    "Commençons déjà par charger et afficher notre environnement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env.seed(1)\n",
    "env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Très bien, nous avons affiché notre environnement.\n",
    "\n",
    "Quelques explications:\n",
    "-  `gym.make()` nous a permis de charger notre environnement\n",
    "-  `env.seed()` nous permet d'avoir les mêmes résultats\n",
    "-  `env.reset()` réinitialise l'environnement et retourne l'état de notre env\n",
    "-  `env.render()` nous permet d'afficher notre env\n",
    "-  `env.close()` ferme l'environnement\n",
    "\n",
    "Maintenant voyons comment nous pouvons interagir avec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env.seed(1)\n",
    "env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(50):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà qui est plus intéressant.\n",
    "\n",
    "Quelques explications:\n",
    "-  `env.action_space.sample()` prend une action au hasard parmi celles possibles\n",
    "-  `state` est votre état après l'action effectuée\n",
    "-  `reward` est la récompense reçu pour avoir effectué cette action\n",
    "-  `done` indique si l'environnement est terminé\n",
    "\n",
    "Le `state` représente l'état de votre agent dans son environnement, dans le cas de l'environnement *MountainCar-v0* le `state` est composé de deux valeurs: `position` et `velocity`.<br>\n",
    "Comme leur nom l'indique, `position` représente la position de l'agent dans l'environnement et `velocity` represente sa velocité à un instant $t$.\n",
    "\n",
    "Regardons comment retrouver ses informations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Etat: \", env.observation_space)\n",
    "\n",
    "print(\"low: \", env.observation_space.low)\n",
    "print(\"hight: \", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décortiquons ces informations ensemble.\n",
    "\n",
    "`Box()` répresente un tableau à N dimensions, il est ici de dimension $(2,)$ ce qui signifie que notre `state` est de dimsensions $(2,)$.<br>\n",
    "Cela correspond bien à (`position`, `velocity`) comme nous l'avons dit précédemment.\n",
    "\n",
    "On a aussi print les `low` et `hight` de ces deux valeurs, ce sont les deux valeurs extrêmes que peut avoir notre `state`.\n",
    "\n",
    "`[-1.2, 0.6]` correspond à l'encradrement des valeurs de `position`.<br>\n",
    "`[-0.07, 0.07]` correspond à l'encradrement des valeurs de `velocity`.\n",
    "\n",
    "Pour en savoir plus vous pouvez voir le repo github de l'environnement [MountainCar](https://github.com/openai/gym/wiki/MountainCar-v0).\n",
    "\n",
    "Maintenant que nous savons comment interpréter les informations de `state`, passons au contrôle de notre agent.\n",
    "\n",
    "**Exercice:** Affichez l'espace d'action de notre agent dans son environnement.<br>\n",
    "**Indice:** [doc de Gym](https://gym.openai.com/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action: \", )  # rajoutez votre code (~1 ligne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultat attendu:** `Action:  Discrete(3)`\n",
    "\n",
    "Interprétons ces informations:\n",
    "\n",
    "`Discrete()` signifie que toutes nos actions $\\in N+$.<br>\n",
    "Le `3` correspond au nombre d'actions effectuables par notre agent dans son environnement, dans notre cas ces actions sont: $Reculer$, $Attendre$ et $Avancer$.\n",
    "\n",
    "# 2 - Le Q-learning\n",
    "\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686\">\n",
    "\n",
    "### Nomenclature:\n",
    "\n",
    "- $Q$ est notre `Q-table`\n",
    "- $St$ représente notre `state` à un instant $t$\n",
    "- $at$ représente l'action prise à un instant $t$\n",
    "- $\\alpha$ (alpha) est le `learning rate`, c'est l'importance que l'on donne au `new state`. <br>Sachant 0 < $\\alpha$ < 1, plus le `learning rate` est élevé, plus on donne de l'importance au `new state`.\n",
    "- $\\gamma$ (gamma) est le `discount factor`, c'est l'importance donnée à un futur reward. <br>Sachant 0 < $\\gamma$ < 1, plus le `discount factor` est élevé, plus notre agent \"pensera\" au long terme.\n",
    "- $\\epsilon$ (epsilon) est le facteur aléatoire de l'apprentissage<br>Sachant 0 < $\\epsilon$ < 1, plus epsilon est élevé, plus notre agent effectuera des actions aléatoires.\n",
    "\n",
    "\n",
    "### Algorithme du Q-learning\n",
    "\n",
    "Pour commencer l'utilisateur définit sa Q-table à des valeur arbitraires.<br>\n",
    "\n",
    "Pour rappel la Q-table contient pour chaque binome `state` $S$ et action $a$, le `reward` qui leur est associé.<br>\n",
    "Si l'utilisateur se trouve dans un `state` $St$ et effectue une action $at$ il recevra en `reward` $Q[St, at]$.\n",
    "\n",
    "Une fois la Q-table créée on commence l'algorithme:\n",
    "\n",
    "L'utilisateur se trouve à un `state` $S$.<br>Il récupère un nombre aléatoire $r$, si $r$ < $\\epsilon$ il effectue une action aléatoire, si $r$ > $\\epsilon$ il effectue alors une action dite `greedy`.<br>\n",
    "Apres avoir effectué l'action $a$, l'utilisateur se trouve dans un `state` $S'$ et a reçu un `reward` $r$.\n",
    "\n",
    "\n",
    "L'utilisateur va pouvoir mettre à jour sa Q-table, pour rappel:\n",
    "$Qnew[st, at] = Q[st, at] + \\alpha * (rt  + \\gamma  * maxQ(st+1, a) - Q[st, at])$\n",
    "\n",
    "Au fur et à mesure que l'utilisateur met à jour sa Q-table, l'agent saura de mieux en mieux comment optimiser ses action pour obtenir le meilleur `reward` et ainsi résoudre son environnement.\n",
    "\n",
    "### La pratique \n",
    "\n",
    "Nous avons vu la théorie, maintenant passons à la pratique.\n",
    "\n",
    "\n",
    "Commençons par initialiser `states` qui contiendra tous les `states` que notre agent pourra rencontrer dans son environnement.\n",
    "\n",
    "Pour rappel:\n",
    "- `[-1.2, 0.6]` correspond à l'encradrement des valeurs de `position`.<br>\n",
    "- `[-0.07, 0.07]` correspond à l'encradrement des valeurs de `velocity`.\n",
    "- Notre agent obtient -1 à chaque action.\n",
    "- Moins notre agent effectu d'action, plus son `reward` sera grand.\n",
    "\n",
    "Une question se pose alors:<br>Il y a une infinité de valeurs dans l'encadrement `[-1.2, 0.6]` et l'encadrement `[-0.07, 0.07]`. Comment toutes les stocker ?<br>\n",
    "\n",
    "Cela n'est pas possible. C'est pourquoi nous allons devoir se contenter d'un échantillons de ces valeurs.<br>\n",
    "La taille de cette échantillon est variable selon la situation, dans notre cas nous allons choisir un échantillon de taille $20$.\n",
    "\n",
    "La taille de notre échantillon est un vecteur important dans le cas du Q-Learning.\n",
    "Plus l'échantillon est grand, plus notre agent sera précis mais plus il prendra du temps à tout explorer.\n",
    "Il y a aussi le facteur mémoire qui rentre en jeu, la taille allouée à notre Q-table augmente de façon exponentielle:\n",
    "- Un échantillon de taille 10 pour 3 actions possibles mènera à une Qtable de shape `((10, 10), 3)` et contenant $300$ valeurs.\n",
    "- Un échantillon de taille 15 pour 3 actions possibles mènera à une Qtable de shape `((15, 15), 3)` et contenant $675$ valeurs.\n",
    "- Un échantillon de taille 20 pour 3 actions possibles mènera à une Qtable de shape `((20, 20), 3)` et contenant $1200$ valeurs.\n",
    "\n",
    "Ici doubler la taille de l'échantillons ne double donc pas la taille de notre Q-table mais la quadruple !\n",
    "\n",
    "Dans notre cas voici à quoi ressemble notre échantillon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES_SPACE = 20\n",
    "\n",
    "POSITION_SPACE = np.linspace(-1.2, 0.6, STATES_SPACE)\n",
    "VELOCITY_SPACE = np.linspace(-0.07, 0.07, STATES_SPACE)\n",
    "\n",
    "print(f\"POSITION_SPACE: {POSITION_SPACE}\\n\")\n",
    "print(f\"VELOCITY_SPACE: {VELOCITY_SPACE}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solution la plus évidente maintenant serait de stocker dans `states[0][0]` la valeur `(-1.2)` et dans `states[0][1]`la valeur `(0.07)` etc. mais cette solution apporterait d'autres problèmes.<br>\n",
    "Exemple: On se retrouve à avoir dans `states[1][0]` la valeur `-1.10526316` mais notre env ne nous retournera probablement pas la valeur exact `-1.10526316` ce qui demanderai a chaque fois d'arrondir cette dernière.\n",
    "\n",
    "Pour contrer cela, au lieu de stocker dans `states` les valeurs de notre échantillon, nous allons stocker les index correspondant aux valeurs de notre échantillons.<br>\n",
    "Exemple: `states[0]` contiendra alors `(0, 0)` correspondant respectivement à $-1,2$ et $-0.07$ dans notre échantillon.\n",
    "\n",
    "**Exercices:** Complétez la fonction `init_states()` pour qu'elle retourne le tableau `states` de taille `STATES_SPACE` contenant toutes les combinaisons des valeurs représentants notre echantillon `(observation, velocity)`.<br>\n",
    "**Indices:**\n",
    "- Uttilisez `STATES_SPACE`\n",
    "- `state[1]` correspondant à `(0, 1)` représente la $1ere$ valeur de notre échantillon de `observation` et la $2eme$ valeur de notre échantillon de `velocity`.\n",
    "- `state[20]` correspondant à `(1, 0)` représente la $2eme$ valeur de notre échantillon de `observation` et la $1ere$ valeur de notre échantillon de `velocity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_states():\n",
    "    states = []\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "\n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return states\n",
    "\n",
    "assert valide_init_states(init_states(), STATES_SPACE), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Complétez la fonction `get_indexes()` pour que depuis un `state` donné en argument elle retourne les indexes correspondants dans notre échantillon.<br>\n",
    "**Indices:** `np.digitize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexes(state):\n",
    "    # rajoutez votre code (~2 lignes)\n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return (position_index, velocity_index)\n",
    "  \n",
    "assert valide_get_indexes(get_indexes((0, 0)), POSITION_SPACE, VELOCITY_SPACE), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant à la gestion des actions de notre agent.\n",
    "\n",
    "Lorsque l'on demande à notre agent d'effectuer une action qui selon lui est optimale, il effectue alors une action dite \"*greedy*\".<br>\n",
    "C'est justement ce que vous allez implémenter.\n",
    "\n",
    "**Exercice:** Complétez la fonction `greedy_step()` pour que depuis un `state` donné elle retourne l'action optimal à faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SPACE = [0, 1, 2]\n",
    "\n",
    "def greedy_step(Q, state, actions=[0, 1, 2]):\n",
    "    # rajoutez votre code (~2 lignes)\n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return action\n",
    "  \n",
    "assert valide_greedy_step(create_testing_Qtable()) == greedy_step(create_testing_Qtable(), (1, 2)), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementons maintenant l'usage d'epsilon.\n",
    "\n",
    "**Exercice:** Complétez la fonction `take_action` pour qu'elle retourne l'action a faire à un moment $t$.<br>\n",
    "**Indices:**\n",
    "- Pour $r$ un nombre aléatoire $\\in$ `N+`, si $r$ < $\\epsilon$ alors l'action effectué sera aléatoire.\n",
    "- `np.random.random()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(epsilon, Q, state, actions=[0, 1, 2]):\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fin de votre code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant passer à l'initialisation de votre Q-table.\n",
    "\n",
    "**Exercice:** Complétez la fonction `init_Qtable() ` pour qu'elle retourne votre Q-table avec toutes ses valeurs initialisées à $0$.<br>\n",
    "**Indice:**\n",
    "- Votre Q-table est de shape `((STATE_SPACE, STATE_SPACE), ACTION_SPACE)`\n",
    "- Qu'est ce qui est de shape `(STATE_SPACE, STATE_SPACE)` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Qtable(states, action_space):\n",
    "    Q = {}\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return Q\n",
    "  \n",
    "assert valide_init_Qtable(init_states(), ACTION_SPACE) == init_Qtable(init_states(), ACTION_SPACE), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que votre Q-table est initalisée vous devez la mettre à jour.\n",
    "\n",
    "**Exercice:** Complétez `Qfunction` pour qu'elle retourne la nouvelle valeur de `Q[state, action` selon la *Q-function*.<br>\n",
    "**Indice:** $Qnew[st, at] = Q[st, at] + \\alpha * (rt  + \\gamma  * maxQ(st+1, a) - Q[st, at])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qfunction(state, action, reward, new_state, alpha, gamma):\n",
    "  # rajoutez votre code (~2 lignes)\n",
    "  \n",
    "  \n",
    "  # fin de votre code\n",
    "  return newQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en situation\n",
    "\n",
    "Tout est en place pour passer à la mise en situation (ou presque, nous y reviendrons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_GAMES = 2_000\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.99\n",
    "epsilon = 1.0\n",
    "EPS_MIN = 0.1\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "episode_score = 0\n",
    "total_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "memory = []\n",
    "\n",
    "states = init_states()\n",
    "Q = init_Qtable(states, ACTION_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Complétez le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for game_number in range(NUMBER_OF_GAMES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = get_indexes(obs)\n",
    "    if game_number % 100 == 0 and game_number > 0:\n",
    "        print('episode ', game_number, 'score ', episode_score, 'epsilon %.3f' % epsilon)\n",
    "    episode_score = 0\n",
    "    while not done:\n",
    "        if (game_number % 500 == 0):\n",
    "            env.render()\n",
    "        # rajoutez votre code (~5 lignes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # fin de votre code\n",
    "        episode_score += reward\n",
    "    total_rewards[game_number] = episode_score\n",
    "    epsilon = max(EPS_MIN, epsilon * EPS_DECAY)\n",
    "\n",
    "print(f\"average reward: {sum(total_rewards) / len(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "\n",
    "for t in range(NUMBER_OF_GAMES):\n",
    "    mean_rewards[t] = np.mean(total_rewards[int(max(0, t - NUMBER_OF_GAMES / 10)): t + 1])\n",
    "\n",
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super, on voit que notre agent apprend bien à résoudre son environnement mais n'y a-t-il pas un moyen d'optimiser son apprentissage ?<br>\n",
    "\n",
    "Si ! Et vous avez déjà vu comment: en utilisant le principe d'*experience replay*.<br>\n",
    "On va rajouter une mémoire à notre agent pour qu'il puisse s'entrainer plusieurs fois sur des situations qu'il pourrait rencontrer seulement rarement.\n",
    "\n",
    "**Exercice:** Complétez la fonction `add_memory` pour qu'à chaque appel elle met à jour la mémoire avec la nouvelle valeur donnée en argument.<br>\n",
    "**Indices:**\n",
    "- Attention à ne pas dépasser `memory_size`\n",
    "- Si on doit remplacer une valeur, on remplace la plus ancienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_memory(memory, state, action, reward, new_state, memory_size=600):\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return memory\n",
    "  \n",
    "def train_on_memory(memory, Q, alpha, gamma):\n",
    "    for mem in memory:\n",
    "        Q[mem['state'], mem['action']] = Qfunction(mem['state'], mem['action'], mem['reward'], mem['new_state'], alpha, gamma)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "episode_score = 0\n",
    "total_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "\n",
    "Q = init_Qtable(states, ACTION_SPACE)\n",
    "memory = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Complétez le code ci-dessous pour que votre agent utilise `memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_number in range(NUMBER_OF_GAMES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = get_indexes(obs)\n",
    "    if game_number % 100 == 0 and game_number > 0:\n",
    "        print('episode ', game_number, 'score ', episode_score, 'epsilon %.3f' % epsilon)\n",
    "    episode_score = 0\n",
    "    while not done:\n",
    "        if (game_number % 500 == 0):\n",
    "            env.render()\n",
    "        # rajoutez votre code (~5 ligne)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # fin de votre code\n",
    "        episode_score += reward\n",
    "    # rajoutez votre code (~1 ligne)\n",
    "    \n",
    "    # fin de votre code\n",
    "    total_rewards[game_number] = episode_score\n",
    "    epsilon = max(EPS_MIN, epsilon * EPS_DECAY)\n",
    "\n",
    "print(f\"average reward: {sum(total_rewards) / len(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "\n",
    "for t in range(NUMBER_OF_GAMES):\n",
    "    mean_rewards[t] = np.mean(total_rewards[:])\n",
    "    mean_rewards[t] = np.mean(total_rewards[int(max(0, t - NUMBER_OF_GAMES / 10)): t + 1])\n",
    "\n",
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Félicitations ! Vous avez réussi votre premier Q-Learning !\n",
    "\n",
    "Essayez de faire de même pour [cette environnement](https://gym.openai.com/envs/CartPole-v1/) 😉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}